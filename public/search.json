[{"categories":null,"content":"Memory Management Colang intends to achieve high throughput and low latency using two strategies: static typing and dispatch, and efficient memory management. In this post we will review common approaches to memory management. The post is background for a later post describing memory management Colang.\nBalance The key challenge in memory management is knowing when some allocation is no longer in use and can be reclaimed. Once some memory is reclaimed it will either be returned to the operating system or recycled in the process to satisfy a subsequent dynamic memory allocation.\nThere are many approaches to memory management today. All approaches must ballance between:\nEfficiency Latency Throughput Safety Efficiency A memory management approach is can be said to be efficient if the majority of memory provided by the hardware can be put to use serving the functional goals of the software. Common causes of inefficient use of memory are overhead, fragmentation, and stale allocations.\nOverhead is caused by additional memory utilization for the sole purpose of the memory management.\nFragmentation occurs when there are ranges of memory which can not be allocated for functional use either because they are too small or due to inefficiency in the allocation strategy.\nStale allocation occur when an allocation is no longer needed for the function of the application but is not available to service new allocation.\nLatency Latency is the number of cycles spent servicing an allocation request.\nThroughput Is the total allocations per unit time an allocator can service. It is not only dependent on latency but also on time spent on reclaim and copying in the case of moving memory manager.\nSafety The property of safety inf a memory manger is how it protects the developer form misusing allocations in a way the leads to functional defects. Used after free, and double free bugs are examples of unsafety in C language memory managers.\nApproaches In the next section we will compare many existing approaches to memory management used today. Each will be described and examined though the lense of efficiency, latency, throughput, and safety.\nStatic Allocation The simplest approach is to use only static allocations. In this strategy no dynamic allocations are used and memory layout is decided at compile time. This is most often used in embedded and safety critical systems, there is zero runtime cost and no concern about allocations failing due to lack of capacity at runtime.\nThe down side is that the developer must know ahead of time how much memory every data structure needs. For systems that interact with outside data sources this is often not possible, and my lead to application errors if assumptions about load are incorrect.\nMemory Efficiency: This depends the application but it is possible to reach very high efficiency as there is no overhead from tracking allocation.\nLatency: There is zero dynamic allocation so the latency is effectively zero.\nThroughput: This is not applicable as there is no allocations.\nSafety: Clearly there can not be a use after free, or double free bug if you never free so there are safety advantages; but to reach higher efficiency it may be that one region might be used for more then one purpose which could lead to serious errors.\nBump Allocation A Bump Allocator uses a very simple strategy where new allocations can be serviced dynamically. The work by tracking a single pointer in to available memory and incrementing the pointer to service each allocation. This leads to a very low latency and high thought allocator but allocations can never be returned except by decrementing the free pointer. In practice most implementations of bump allocator only support resetting the free pointer to its initial state reclaiming all allocations service as a single operation.\nMemory Efficiency: Bump allocators have very low overhead but often lead to a large number of stale allocations leading to low efficiency.\nLatency: Latency is very low as allocations can be serviced by a single increment operation.\nThroughput: Bump allocators achieve very high throughput.\nSafety: The lack of reclaim in bump allocators removes most allocator related unsafety issues.\nManual Memory Management In the C language memory management is entirely manual; the programmer must explicitly ask memory to be allocated, and manually free it once it is no longer in use. This approach allows for complete control. The advantage is that the engineer can pick the trade off between resource efficiency and performance that fits the needs of there application.\nFor maximum performance all memory allocations can preformed prior to performance critical code, and all reclamation can be deferred until after a performance critical section. This allows carefully written code minimize latency.\nThe down side of manual memory management is that it is practically impossible to do safely. Use after free, double free, and other such bugs are common reasons for security critical defects, as well as crashes and memory leeks which reduce availability. Given it’s difficulty manual memory management also imposes a high cost in the development cycle where time must be spent verifying correctness, and reworking code when issues are found (often after release).\nMemory Efficiency: This approach allows very efficient use of available memory. Some issues with memory fragmentation but can occur and some overhead exists for tracking free pages suitable for allocation. Over with manual management excellent efficiency is possible.\nLatency: Allocates in manually managed languages tend to be low latency, but they do have a measurable cost. An advantage is that in languages like C, one can pick the allocator used. Because of this it is possible to select an allocator the meets the specific needs of the application.\nThroughput: Over all allocates in manually managed languages tend have good throughput, but have a measurable cost. An advantage is that in languages like C, one can pick the allocator used. Because of this it is possible to select an allocator the meets the specific needs of the application.\nSafety: Safety depends on developers. Many tools and technics have been developed to aid developers, such as static analyzers and fuzzers, but they only partially mitigate the the inherent unsafety of manual memory management.\n(Automatic) Reference Counting The use of reference counting was popularized in the 90’s by languages like TCL, Perl, and Python. It has seen recent resurgence in languages like ObjectiveC and Swift.\nIt works by keeping a counter for each allocation, incrementing the counter when a new reference to an allocations is created and decremented each time a reference is retired. When the reference count of an allocation becomes zero it is considered unused and is reclaimed.\nThe main advantage of reference counting is that it takes the responsibility of reclaiming memory from the developer, greatly increasing safety.\nReclamation is immediate and incremental keeping some of the best properties of manual memory management.\nLanguages that use reference counting usually do not allow pointer arithmetic or casting to pointers. This is necessary as this could result in uncounted references leading to use after free bugs.\nOne common issue with reference counting it that if the program ever creates a reference cycle, where directly or transitively, an allocation holds a reference to itself, the reference count may never go to zero and the memory will never be reclaimed.\nIn practice the greater issue with reference counting is the performance cost. In code that takes many short lived references the reference counter is constantly incremented and decremented. Where each of these operations are fast, over all the effect on whole program performance can be significant.\nMemory Efficiency: Reference counting memory allocators will have similar efficiency to manually managed memory with the difference being the in the need to increase allocation sizes to allow for the counter.\nLatency: Reference counting tend to be low latency. This cost will be similar to allocations in manually managed languages; but unlike them one can not usually switch out the allocator.\nThroughput: This approach takes a hit in throughput due to the cost of managing the counter. It should be noted that this cost is not payed during allocation and reclamation but during use of the allocated memory and so effects overall application performance.\nSafety: Over all safety is good; removing most of the errors that lead to unsafety in memory management. One caveat is that because circular reverences prevent collection, memory may leek over time leading to to memory exhaustion.\nMark and Sweep Mark and sweep is a reclaim strategy which works by starting with know live allocations, referred to as roots, and scanning each allocation for reference to other allocations. This process is repeated until no new allocation are found. At this point any outstanding allocations which were not visited during the process must be unused and can be reclaimed.\nIn contrast to reference counting, mark and sweep will collect allocations with cyclic references and avoids the maintenance cost of the counter.\nAs with reference counting, languages that use reference counting usually do not allow pointer arithmetic or casting to pointers. This is necessary as this could result in uncounted references leading to use after free bugs.\nOne complication with mark and sweep collectors is that in there simple form, new allocations can not be made concurrently with collection. If this were avowed the new allocations may not be visited and reclaimed while they are still in use.\nBecause of this many mark and sweep collectors pause the application while garbage collection is running. There are implementations called concurrent mark and sweep which do not require the main program to be paused during collection, but they impose additional complexity and throughput costs.\nA second artifact of a mark and sweep garbage collector is that memory is not reclaimed until the garbage collection is run. This might be long after a allocation is no longer in use. Because of this delay surprising effects in languages which support object destructors as it can not predicted when or if a destructor will ever be called.\nMemory Efficiency: Mark and sweep memory allocator will have similar efficiency to manually managed memory.\nLatency: Over all allocates used with mark and sweep tend to be low latency, but they do have a cost. This cost will be similar to allocations in manually managed languages; but unlike manually managed allocation one can not usually switch out the allocator.\nWhen a using a stop the world collector, pauses add an additional unpredictable latency the the application.\nThroughput: There is a wide verity of implementations of mark and sweep collection but over all collectors which stop the world tend to have higher throughput at the cost of latency, and those that have current collectors achieve lower latency at the cost of throughput.\nSafety: Safety is good removing most of the errors that lead to unsafety in memory management.\nCompacting Garbage Collection Compacting garbage collectors try to increase memory efficiency by moving allocations into contiguous memory ranges during garbage collection, reducing fragmentation.\nThe cost of compaction is that memory must be copied and pointers must be updated to reference new memory locations.\nMemory Efficiency: Compacting can have positive impacts on efficiency by removing fragmentation.\nLatency: Compacting can reduce allocation latency but pauses due to garbage collection may get longer.\nThroughput: The time spent moving memory during compaction negatively effects throughput.\nSafety: Compaction dose not have a positive or negative effect on safety.\nGenerational allocation Generational allocation is a variation of a compacting garbage collector. They are based on the observation that most allocations have short life times. Using this idea it divides the allocations in to new and old types. All allocations start as new. The new allocator is optimized for latency and throughput at the cost of efficiency. When a collection cycle occurs, all in use new allocations are moved to the old generation. THe old generation allocator uses an strategy which makes more efficient use of memory at some cost to latency and throughput.\nMemory Efficiency: This approach keeps much of the efficiency gains of compaction, and should have similar advantages.\nLatency: Allocation latency in generational allocation is very low; but may be paused if a collection cycle is occurring.\nThroughput: The time spent moving memory during compaction negatively effects throughput, but new generation allocation is very high thruput; resulting is good throughput for bursty allocation patters.\nSafety: Compaction dose not have a positive or negative effect on safety.\nStatic Lifetime Analysis An approach that has gained reascent popularity is static life time analysis. This approach is most like manually memory management, but instead of the developer being responsible for deciding when an allocation is not needed and manually freeing memory, the compiler uses static analysis to automatically insert calls to free the memory. This approach was brought to public attention by the Rust programming language but also exists C++ as RAII, and is also implemented by linear types in langues like AST.\nMemory Efficiency: This approach like manual memory management puts the developer in control allowing for very high efficiency.\nLatency: Latency will match that of manual memory management and will be dependent on the underlying allocator.\nThroughput: Like Latency throughput will match that of manual memory management and will be dependent on the underlying allocator.\nSafety: The use of static analysis can completely remove the safety issues associated with manual memory management making this approach very safe.\nCollections Data Structures An potentially surprising entry in the list of memory management approaches are data structures. The type of structures we are talking about are ones that hold many values such as list, maps, and arrays. High performance implementations of these data structures must carefully arrange stored records to minimize overhead and maximize performance.\nA simple example of this is automatically resized arrays. These are vectors of values which support indexing and iteration. A common way to implement this structure to have a underlying fixed size array of liner memory. THe capacity of the array is dabbled anytime its capacity is exceeded. This doubling is preformed by allocating a new array and then copying the existing values in to the new array. The original backing array is then reclaimed.\nWhile the data structure is built on top of a second allocator it also acts as one.\nCollections implementations must know is when it is safe to free any old backing memory. In the case where only small values are stored it may be posable to alway resolve reads by value, but if it is necessary to return references to records or use the collection in concurrently the collection must implement reference counting, life time analysis, etc must be used.\nMemory Efficiency: The reason that collections implement custom allocation strategies is increase efficiency so they are usually quite efficient for their use case.\nLatency: Latency will also often be very good if the appropriate data structure is picked for the workload.\nThroughput: Likewise throughput will often be good if the data structure chosen matches the workload.\nSafety: The safety of this approach based on the API provided which is often influenced by the underlying memory management approach. Overall there is not a single answer to the question of safety for collections and it must be answered for each collection independently.\nConclusion We have described many different approaches to memory management, each with different goals, costs, and benefits. When we detail the Colang design it will be a mixture of these approaches optimized for the Colang execution model.\n","description":"","tags":null,"title":"Memory Management","uri":"/posts/memory-mangment/"},{"categories":null,"content":"Why Colang There is a gap in the current popular languages being used. Most services APIs are still written in high level languages from the 90’s. These were designed based on lessons and assumptions about computing thirty yeas ago.\nWe are seeing lots of innovation in programing language design, Rust, Swift, Kotlin, and Go have see lots of growth and are supported by large corporations; others such Zig and Nim have popular fallowing and are growing. One thing all these languages have in common is that they were built largely as alternatives enterprises or systems languages:\n- C++ -\u003e Rust - ObjectiveC -\u003e Swift - Java -\u003e Kotlin - C -\u003e Go This is not to say each had no other goals, or that in all cases the replacement is one to one; but overall the current innovation is around languages in this domain.\nWhere systems and enterprise languages are important, much of the code written today is to build service APIs and written in high level languages such as JavaScript, Python, and PHP. The high level language category has seen some innovation with examples like TypeScript and Elm. Both TypeScript and Elm work to provide correctness and offer improvements in developer esperance, but both in practice are largely JavaScript replacements targeted at in browser applications.\nFor all the code that runs on servers and not written in a systems language, we have seen little innovation.\nCode which runs on servers is important code. It costs organizations money to run, the time it takes to run effects latency for clients, and avoided computation can lead to a reduction in carbon emissions. For all these reasons I think it is worth trying to improve the performance and efficiency of the code we run on servers.\nWhy are high level languages inefficient Todays popular high level languages, PHP, Python, JavaScript, are all dynamically typed, garbage collected languages, which use heap allocations for most types.\nAt the time they were designed most commercial code was written in C or Pascal, and their features made them much easer to learn, safer, and offered quicker to development. They were also all slow; but at the time, in late 80’s early 90’s, the web was starting to take off and the single threaded performance of computers was doubling every eighteen months. In this environment it was more effective to optimism for developer time rather than execution performance.\nToday we have to live with there inefficiencies but are no longer getting a free ride from the work of CPU manufactures. Languages like JavaScript have made great strides in in performance but at the cost of complex multi stage optimizing JIT compilers; but even after this work JavaScript is still significantly slower that low level languages.\nThe source of the the inefficient execution comes down to two design choices. First the use of general purpose garbage collection, and second the use of dynamic typing a dispatch.\nAnother Approach In fallowing posts we will describe a language architecture we call Colang. Colang attempts to address both the inefficiencies of general purpose GC, as well as dynamicism.\nThe key design choices are:\nThe language runs in an event loop. Initial allocations use a bump allocator. Garbage collection only happens after exiting to the event loop. All long lived allocations are owned by collections. It will be high level but largely static language. Why Event Loops The use of an event loop is driven by a few observations.\nEvent loops have been successful in languages such as javascript, and more generally event driven systems.\nEvent loops compatible with async programming which is important for high request concurrency.\nLastly, and more importantly, when execution exits back to the event loop, the run time is provided clear and convent time to preform garbage collection which will never stall active work.\nWhy a bump allocator Bump allocators have been shown to be very efficient new generation allocators in generational garbage collectors such as the JVM. They require almost no book keeping, and most new allocations are ephemeral meaning in practice most memory never has to be moved.\nWhy collections for long lived allocations We observe that efficient collections (vector, list, map, etc) each implements their own memory management strategy. This is true even in garbage collected languages. We further assert, with out evidence, that the majority of long lived allocations are held in a collection. It is there for natural to manage log lived allocations using the memory management implemented by collection which owns it.\nWhy Static We hypotheses that slow performance in high level languages is due not to being highly expressive but due to being dynamic.\nDynamic dispatch require runtime checks and prevent the complier from preforming many optimizations. Advanced JITs, such as JavaScript runtimes found in browsers, try to avoid the penalty dynamicism by assuming that types don’t change; but JavaScript engines must still include checks for when type guesses are wrong, and the checks fail they must recompile.\nDynamic types require checks on every use which impacts reliability as it results in runtime type errors, and like dynamic dispatch, dynamic types also prevent the use of common complier optimizations.\nConclusion Colang is a language architecture, and hopefully one day a language, which attempts to be a high level language for weighting services in. It’s main goal is to provided a developer experience on par with languages like python, JavaScript, and PHP.\nIt is not meant to be a systems language, but strives give up little performance compared to a systems language.\nIn future posts we will explore in more detail the approach to execution and garbage collection, as well as consider the design of the type system.\n","description":"","tags":null,"title":"Why Colang","uri":"/posts/why-colang/"},{"categories":null,"content":"Entanglement, Space Only Relations, and Black Hole Information Are entangled partials connected in space but not time, and if so is that how information escapes the event horizon of a black hole?\nEntanglement When a pair of entangled partials are created they create a unique configuration of the universe. In most configurations we experience information is carried though space-time as excitations of some field, higgs, electro magnetic, etc; but entangled partials seems to propagate information though space only and not time.\nWe see this in the instantaneous nature of “spooky action at a distance” as well as the apparent back propagation of information thought time seen in the delayed choice quantum erasure experiments. What ever the relation that holds entangled partials together it seems to be one that interacts with spae but not time.\nBlack Holes One interpretation of why escape is impossible from a black hole is that the the stretching of space to infinity stops the passage of time, and it is this stoppage of time that makes black holes a perfect trap.\nHawking Radiation But black holes are not a perfect trap. If a pair of victual particles is instantiated at the event horizon, it is possible that one falls into the black hole and the other escapes into the universe. This results in the black hole loosing mass leading to it’s eventual evaporation.\nBut these virtual particles are brought in to existence entangled, with one ending up inside and one outside the black hole. If entanglement is a cupping in space but not time, might it be possible that entangled partial inside the black hole can freely communicate information out to its sibling as their communicate is not trapped by stopped time?\n","description":"","tags":null,"title":"Entanglement Is Space Only","uri":"/posts/entanglement-is-space-only/"},{"categories":null,"content":"Post Labor Timeline predictions of breakthrough technology are notoriously unreliable; but I think it is worth thinking about them before they arrive. In this post we will discuss the idea that automation will lead to a post labor economy in which labor is no longer required to turn capital into wealth. This upends much of the macro economic theories that underpin both socialist and capitalist systems.\nDecoupling One might argue that automation has played the role of driving labor out of industries for centuries.\nEuropean farming in the 1800s accounted for well over 50% of the work force where today it is around 3%. This drop is not driven just by growing populations but by drastic reductions in the number of people employed in the industry. This was a realignment of the economy but it was accompanied by a lateral move with most of the workforce switching into services and industry.\nWhere today’s automation is different, might it still just be a move of the workforce and not an end to labor? I believe the answer is no, this change is different.\nSince its recovery from the great recession (2007-2009) the United States has seen a sharp decoupling of productivity, employment, and wages. This effect has been dubbed The Great Decoupling\nI believe this decoupling is driven by a shift from automating mechanical processes to automating knowledge.\nKnowledge Automation The development of knowledge technology has progressed along the path of: oral traditions, written language, the printing press, bureaucracy, information technology (IT), and now AI. Each change led to new efficiencies which drove productivity, but a significant change happened with the introduction of information technology. Prior to the introduction of IT, each development allowed people to better record, disseminate, and organize information - but information technology was the automation of data processing.\nOne of the largest effects of automation of data processing was the collapse of the large bureaucracy that historically had managed information. Bureaucracies processed information with people and filing systems which were replaced by database systems. A database could replace entire buildings of people and filing cabinets with a handful servers and people. Even more astounding: the rate of improvement in our IT systems, following Moore’s Law, were doubling in capability every 24 months.\nThe weakness of databases and IT systems was that they worked only on information and not knowledge. With information one can sum a number, count words, and average balances; but these are all quantitative questions. In contrast, questions of knowledge are categorical.\nInformation is quantitative where knowledge is categorical.\nThe power of knowledge The power of categorical information is transformative. Image generation systems such as OpenAI’s DALL-E let us ask for images with english prompts with impressive results:\n“A realistic painting of a robot lawyer arguing for AI rights to the supreme court of the united states.”\nHere DALL-E “understood” the catagories of “painting”, “robot”, “arguing”, and “United States court”, etc.\nChatGTP, also from OpenAI, can generate text responses instead of images:\nprompt: Tell me a story of how spirits gave humans the knowledge of spinning yarn\nLong ago, in a time before humans knew how to spin yarn, the spirits of the earth, air, water, and fire came together to teach them this valuable skill.\nThe earth spirit taught them how to collect and prepare the fibers from plants and animals, showing them which plants had the strongest and most durable fibers, and how to extract and clean the fibers so they could be used to make yarn.\nThe air spirit taught them how to spin the fibers into yarn using a spinning wheel, showing them how to hold the fibers tightly and how to use the wheel to twist the fibers into a strong and consistent yarn.\nThe water spirit taught them how to dye the yarn using natural dyes from plants and flowers, showing them how to mix the dyes to create different colors and how to use the dyes to add color and pattern to the yarn.\nFinally, the fire spirit taught them how to use the yarn to weave and knit fabrics, showing them how to use looms and needles to create beautiful and functional garments, blankets, and other items.\nWith the help of the spirits, humans learned how to spin yarn and use it to create all kinds of useful and beautiful things. And they were forever grateful to the spirits for sharing their knowledge and wisdom with them.\nHere ChatGTP understands the category of “a story about spirits”, and “giving gifts”, and of “fiber crafts”.\nIn both these examples the results are synthesized from learned catagories. This is a fundamental shift for the types of quantitative questions and answers that computer systems are capable of.\nSystems like DALL-E and ChatGTP are impressive, but progress in this field is not new. Automated legal discovery has allowed us to replace teams of lawyers, and in the medical field computer-aided diagnosis is a decades old field of research.\nAutomated knowledge is already here and a growing driver for productivity.\nAutomated Knowledge effects on labor Knowledge automation is fundamentally different than automating a procedural task. Each time we automate a procedure it requires a new machine: a thresher for separating wheat from chaff, an loom to make cloth, or a train to move goods. In contrast, when we build knowledge systems, we are limited by the need to collect examples to train on. The machine can largely remain unchanged while working in a large number of different domains. As we have more advanced knowledge systems their effects are cross cutting, affecting the entire economy and not one vertical at a time.\nPostulating about the future, we can imagine a time when almost no people are required to turn some capital - a farm, a mine, or a factory - into wealth, resulting in a dystopia where a small percentage of the population receives the bounty of the planet while the vast majority are left to survive on scraps.\nThis may not be the only option though.\nAutomation enables delivering goods efficiently without needing to scale production. Corporations reduced their head count when they moved from filing cabinets to database servers. CNC tools are making it economical to run small production manufacturing in the U.S. again. And the web, and automated package handling has given new life to small manufacturers and artisans.\nThe current requirement of scale drives the concentration of capital into the hands of the few. If automation reverses this trend, we may avert the dystopian outcomes of vastly unequal holding of wealth.\nConclusion As automaton of knowledge continues, there will be a dramatic shift in society as fewer and fewer people are needed in the labor market. It is my expectation that this will happen simultaneously across industries leaving few chances for people to seek new employment in different jobs. This will necessitate new social contracts and a change in our culture’s relation to work.\n","description":"","tags":null,"title":"Post Labor","uri":"/posts/post-labor/"}]
